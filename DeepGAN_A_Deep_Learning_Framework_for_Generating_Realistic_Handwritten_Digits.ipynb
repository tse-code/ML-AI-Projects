{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjJDT9T01nnc2lQCqKxqPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tse-code/ML-AI-Projects/blob/main/DeepGAN_A_Deep_Learning_Framework_for_Generating_Realistic_Handwritten_Digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-HM-_Zg_XF8E"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
        "from tensorflow.keras.models import Model, Sequential  # Ensure Sequential is imported from keras.models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "NOISE_DIM = 100  # Dimension of noise vector\n",
        "BATCH_SIZE = 64  # Batch size\n",
        "EPOCHS = 5000  # Number of epochs\n",
        "HALF_BATCH = BATCH_SIZE // 2  # Half batch for training discriminator\n"
      ],
      "metadata": {
        "id": "Df_w9N8jXIYv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "adam = Adam(learning_rate=0.0002, beta_1=0.5)"
      ],
      "metadata": {
        "id": "d2qFaqt-XMDv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(NOISE_DIM,)))  # Use Input() explicitly to avoid warnings\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(28 * 28 * 1, activation='tanh'))\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "    return model"
      ],
      "metadata": {
        "id": "3dfnSkfXXSCO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(28, 28, 1)))  # Use Input() explicitly\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "lQGRZUQQXSyK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GAN Model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator for GAN training\n",
        "    gan_input = Input(shape=(NOISE_DIM,))\n",
        "    generated_img = generator(gan_input)\n",
        "    gan_output = discriminator(generated_img)\n",
        "    model = Model(gan_input, gan_output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "    return model"
      ],
      "metadata": {
        "id": "pdTg9WsVXZDb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the generator\n",
        "generator = build_generator()\n",
        "\n",
        "# Build the GAN\n",
        "gan = build_gan(generator, discriminator)"
      ],
      "metadata": {
        "id": "0MfphIFvXcgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the generator\n",
        "generator = build_generator()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZWLIrckdXdWa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the GAN\n",
        "gan = build_gan(generator, discriminator)"
      ],
      "metadata": {
        "id": "ilssMtvEaA5G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (for example, MNIST)\n",
        "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train / 127.5 - 1.0  # Normalize images between -1 and 1\n",
        "X_train = np.expand_dims(X_train, axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h8SRpjBGXgf4",
        "outputId": "e0860b76-996b-49c4-ff9d-49283edd2931"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_d_loss = 0.0\n",
        "    epoch_g_loss = 0.0\n",
        "\n",
        "    for _ in range(X_train.shape[0] // BATCH_SIZE):\n",
        "        # Step 1: Train Discriminator on real and fake data\n",
        "        idx = np.random.randint(0, X_train.shape[0], HALF_BATCH)\n",
        "        real_imgs = X_train[idx]\n",
        "        noise = np.random.normal(0, 1, (HALF_BATCH, NOISE_DIM))\n",
        "        fake_imgs = generator.predict(noise)\n",
        "\n",
        "        real_y = np.ones((HALF_BATCH, 1)) * 0.9  # Label smoothing\n",
        "        fake_y = np.zeros((HALF_BATCH, 1))\n",
        "\n",
        "        # Train discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(real_imgs, real_y)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_y)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Step 2: Train Generator\n",
        "        noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_DIM))\n",
        "        ground_truth_y = np.ones((BATCH_SIZE, 1))  # Generator tries to fool the discriminator\n",
        "\n",
        "        discriminator.trainable = False  # Freeze discriminator for generator training\n",
        "        g_loss = gan.train_on_batch(noise, ground_truth_y)\n",
        "\n",
        "        # Ensure losses are scalar before adding\n",
        "        epoch_d_loss += d_loss[0] if isinstance(d_loss, list) else d_loss\n",
        "        epoch_g_loss += g_loss if isinstance(g_loss, (float, int)) else g_loss[0]\n",
        "\n",
        "    # Average the losses for the epoch\n",
        "    avg_d_loss = epoch_d_loss / (X_train.shape[0] // BATCH_SIZE)\n",
        "    avg_g_loss = epoch_g_loss / (X_train.shape[0] // BATCH_SIZE)\n",
        "\n",
        "    d_losses.append(avg_d_loss)\n",
        "    g_losses.append(avg_g_loss)\n",
        "\n",
        "    # Print losses for each epoch\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Discriminator Loss: {avg_d_loss}, Generator Loss: {avg_g_loss}\")\n",
        "\n",
        "    # Save the generator model every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        generator.save(f'generator_epoch_{epoch+1}.h5')\n",
        "\n",
        "    # Display generated images every 500 epochs\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        display_images(generator)\n",
        "\n",
        "\n",
        "    # Function to display generated images\n",
        "def display_images(generator, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, NOISE_DIM))\n",
        "    generated_imgs = generator.predict(noise)\n",
        "    generated_imgs = 0.5 * generated_imgs + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_imgs[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Plot loss graph after training\n",
        "def plot_loss(d_losses, g_losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(d_losses, label=\"Discriminator Loss\")\n",
        "    plt.plot(g_losses, label=\"Generator Loss\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Call plot_loss function to display the loss graph\n",
        "plot_loss(d_losses, g_losses)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WNZQl5nGXjZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}